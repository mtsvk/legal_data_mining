# -*- coding: utf-8 -*-
"""pipeline_legal_data_mining_20250725.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umCojUmc9JbCpT3RoZ9MogLfqoBTWVhQ

# ü§ñ **Pipeline de Miner√≠a de Datos Jur√≠dicos con Embeddings y Modelos Generativos**
Este notebook ofrece un flujo de trabajo completo y reproducible para analizar fichas jurisprudenciales o res√∫menes de sanciones con t√©cnicas de miner√≠a de texto y machine learning. Como estudio de caso, procesa sanciones por infracciones al GDPR extra√≠das de enforcementtracker.com (CMS Law.Tax, CC BY-NC-SA 4.0), pero es f√°cilmente adaptable a cualquier otro dataset de textos jur√≠dicos o administrativos.

# Script 00 - Configuraci√≥n Inicial
"""

# =========================================================================
# Script 0 ‚Äî Configuraci√≥n inicial reproducible (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace este script?
# Objetivos:

#   - Instalar todas las dependencias necesarias para el pipeline.
#   - Fijar la semilla de los experimentos para garantizar reproducibilidad.
#   - Guardar en disco una configuraci√≥n global (config.json) que otros notebooks puedan recargar.
#   - Verificar la conexi√≥n con la API de OpenAI y la generaci√≥n de embeddings (sanity‚Äêcheck).
#
# ¬øPor qu√©?
#   - Asegura que cualquiera que abra este notebook use exactamente las mismas versiones de librer√≠as y par√°metros.
#   - Facilita la traza de los experimentos: puedes volver a correrlos con la misma semilla.
#   - Detecta desde el inicio problemas de conexi√≥n o credenciales con OpenAI.
#
# Requisitos previos:
#   1. Python ‚â• 3.8 (Colab ya lo proporciona).
#   2. OpenAI API Key:
#      ‚Ä¢ Genera el token en https://platform.openai.com ‚Üí API Keys ‚Üí Create new secret key.
#      ‚Ä¢ Permite autenticar y usar servicios de OpenAI (embeddings, GPT, etc.).
#      ‚Ä¢ Gu√°rdalo en un `.env` o introd√∫celo interactivamente.
#
# Librer√≠as externas y su funci√≥n:
#   - openai         : Cliente oficial de la API de OpenAI.
#   - python-dotenv  : (Opcional) Carga variables de entorno desde un `.env`.
#   - tqdm           : Barras de progreso (√∫til en batchs).
#   - matplotlib     : Gr√°ficos (usado m√°s adelante).
#   - scikit-learn   : Algoritmos de clustering, SVD, m√©tricas.
#   - joblib         : Serializaci√≥n de modelos (p.ej. KMeans).
#
# Librer√≠as est√°ndar de Colab (preinstaladas):
#   - numpy, pandas : C√°lculos num√©ricos y manejo de tablas.
#   - os, sys, platform, random, json, time : Utilidades del sistema.
#
# Flujo del Script:
#   1) Instalar dependencias.
#   2) Importar librer√≠as y chequear versiones.
#   3) Fijar semilla para reproducibilidad.
#   4) Crear directorio `outputs/`.
#   5) Obtener OpenAI API Key.
#   6) Guardar par√°metros en `outputs/config.json`.
#   7) Sanity‚Äêcheck de embeddings: generar y guardar vectores de prueba.
# =========================================================================

# 1) Instalaci√≥n de dependencias externas clave
!pip install --quiet openai python-dotenv tqdm matplotlib scikit-learn joblib

# 2) Imports y chequeo del entorno de ejecuci√≥n
import os
import sys
import platform
import random
import json
import time

import numpy as np
import pandas as pd
from getpass import getpass

print("‚úÖ Versi√≥n de Python:", sys.version.split()[0])
print("‚úÖ Plataforma       :", platform.platform())
print("‚úÖ Numpy            :", np.__version__)
print("‚úÖ Pandas           :", pd.__version__)

# 3) Fijar semilla de experimentos para reproducibilidad
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# 4) Crear directorios de trabajo
BASE_DIR   = "/content"
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 5) Obtener OpenAI API Key
#    - Si usas python-dotenv, aseg√∫rate de tener un .env con OPENAI_API_KEY
#    - Si no, introd√∫cela de forma interactiva:
if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = getpass("üîë Ingresa tu OpenAI API Key: ")

# 6) Guardar configuraci√≥n global en JSON
CONFIG = {
    "seed": SEED,
    "embedding_model": "text-embedding-ada-002",
    "batch_size": 64,
    "max_retries": 5,
    "backoff_base": 2,
    "dataset_csv": "gdpr_fines.csv",
    "text_column": "summary_tfidf",
    "output_dir": OUTPUT_DIR
}

config_path = os.path.join(OUTPUT_DIR, "config.json")
with open(config_path, "w") as f:
    json.dump(CONFIG, f, indent=2)

print(f"üó∫Ô∏è Configuraci√≥n guardada en: {config_path}")

# 7) Sanity‚Äêcheck de embeddings con OpenAI
from openai import OpenAI

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

def embed_batch(texts):
    """
    Genera embeddings con backoff exponencial ante errores.
    Par√°metros:
      texts: lista de strings a convertir.
    Retorna:
      lista de vectores (listas de floats).
    """
    for attempt in range(CONFIG["max_retries"]):
        try:
            resp = client.embeddings.create(
                model=CONFIG["embedding_model"],
                input=texts
            )
            return [d.embedding for d in resp.data]
        except Exception as e:
            wait = CONFIG["backoff_base"] ** attempt
            print(f"‚ö†Ô∏è {e}. Reintentando en {wait}s...")
            time.sleep(wait)
    raise RuntimeError("‚ùå Todos los reintentos de embedding fallaron")

# 8) Prueba de embeddings: generar y guardar vector de ejemplo
test_texts = [
    "GDPR prueba: transparencia y seguridad.",
    "Notificaci√≥n de brecha de datos."
]

try:
    embs = embed_batch(test_texts)
    arr  = np.array(embs, dtype=np.float32)
    np.save(os.path.join(OUTPUT_DIR, "embedding_prueba.npy"), arr)
    print("‚úÖ Embeddings OK ‚Äî forma:", arr.shape)
    print("üíæ Guardado: outputs/embedding_prueba.npy")
except Exception as e:
    print(f"‚ùå Error en prueba de embeddings: {e}")

# 9) Nota de uso
print("\n‚ÑπÔ∏è Para recargar esta configuraci√≥n en otro notebook:")
print(f"   with open('{config_path}') as f:\n       CONFIG = json.load(f)")

"""# Script 01 - Ingesta y Limpieza"""

# ==============================================================================
# Script 1 ‚Äî Ingesta y limpieza b√°sica reproducible (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace este script?
# Objetivos:
#   - Leer el CSV de datos originales.
#   - Renombrar columnas a snake_case para facilitar su uso en Python.
#   - Eliminar duplicados exactos y registros sin texto en la columna principal.
#   - Guardar un archivo Parquet limpio para las siguientes etapas.
#
# ¬øPor qu√©?
#   - Asegura que el dataset est√© estructurado y libre de ruido b√°sico.
#   - Parquet es m√°s r√°pido y eficiente que CSV para lecturas posteriores.
#
# Requisitos:
#   - Tener ejecutado el Script¬†0 para generar `outputs/config.json`.
#   - Haber subido el CSV (p.ej. `gdpr_fines.csv`) a `/content`.
#
# Flujo:
#   1) Carga configuraci√≥n global.
#   2) Leer el CSV original y mostrar el conteo inicial.
#   3) Renombrar columnas con espacios/caracteres especiales.
#   4) Eliminar duplicados y filas sin texto en 'summary'.
#   5) Guardar el DataFrame limpio en Parquet.
# ==============================================================================

import os
import json
import pandas as pd

# 1) Carga de configuraci√≥n
cfg         = json.load(open('/content/outputs/config.json'))
BASE_DIR    = '/content'               # Carpeta ra√≠z en Colab
OUTPUT_DIR  = cfg['output_dir']        # Carpeta de salida configurada en Script¬†0
DATASET_CSV = cfg['dataset_csv']       # Nombre del archivo CSV de entrada

# 2) Leer CSV original
csv_path = os.path.join(BASE_DIR, DATASET_CSV)
df = pd.read_csv(csv_path)
print(f"üìä Filas originales: {len(df)}")

# 3) Renombrar columnas a snake_case
mapping = {
    'Authority': 'authority',
    'Date': 'date',
    'Fine (‚Ç¨)': 'fine_eur',
    'Controller/Processor': 'entity',
    'Sector': 'sector',
    'Quoted Art.': 'quoted_art',
    'Type': 'violation_type',
    'Summary': 'summary'
}
df = df.rename(columns=mapping)
print("üîÑ Columnas renombradas:", df.columns.tolist())

# 4) Eliminar duplicados y registros sin 'summary'
before = len(df)
df = (
    df.drop_duplicates(subset=['authority','date','entity','fine_eur','summary'])
      .dropna(subset=['summary'])
)
print(f"‚úÇÔ∏è Eliminadas {before - len(df)} filas; quedan {len(df)}")

# 5) Guardar el DataFrame limpio en Parquet
clean_path = os.path.join(OUTPUT_DIR, 'gdpr_clean.parquet')
df.to_parquet(clean_path, index=False)
print("üíæ Parquet limpio guardado en:", clean_path)

# ‚ÑπÔ∏è Para replicarlo con otro dataset:
#   ‚Ä¢ Actualiza 'dataset_csv' en outputs/config.json.
#   ‚Ä¢ Adapta 'mapping' a tus nombres de columna.
#   ‚Ä¢ Cambia el 'subset' de drop_duplicates seg√∫n tus llaves √∫nicas.

"""# Script 02 - Limpieza Avanzada y Stopwords"""

# =========================================================================
# Script 2 ‚Äî Limpieza avanzada y stopwords reproducible
#               (para Google Colab 2024-25)
#
# ¬øQu√© hace este script?
# Objetivos:
#   - Eliminar etiquetas HTML y URLs del texto.
#   - Normalizar caracteres Unicode y pasar a min√∫sculas.
#   - Eliminar stopwords gen√©ricas (ingl√©s), de dominio espec√≠fico, de agencias DPA, gentilicios y ruido de clusters.
#   - Filtrar din√°micamente t√©rminos que aparecen en m√°s del 70% de los documentos.
#   - Generar una nueva columna `summary_tfidf` lista para vectorizaci√≥n TF-IDF.
#
# Requisitos:
#   - Haber corrido el Script 0 y el Script 1 para disponer de:
#       ‚Ä¢ outputs/config.json
#       ‚Ä¢ outputs/gdpr_clean.parquet
#   - Librer√≠a `scikit-learn` instalada (v√≠a pip en Script 0).
# =========================================================================

import os
import re
import html
import unicodedata
import json
import pandas as pd
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from sklearn.feature_extraction.text import TfidfVectorizer

# 1) Cargar configuraci√≥n y datos
t_cfg = json.load(open('/content/outputs/config.json'))
OUTPUT_DIR = t_cfg['output_dir']
clean_parquet = os.path.join(OUTPUT_DIR, 'gdpr_clean.parquet')
df = pd.read_parquet(clean_parquet)
print(f"üì• Le√≠do Parquet limpio: {clean_parquet}  (n={len(df)})")

# 2) Funci√≥n de limpieza robusta
def clean_text(txt):
    """
    - Quita etiquetas HTML.
    - Elimina URLs.
    - Normaliza Unicode (NFKC) y pasa a min√∫sculas.
    - Reemplaza m√∫ltiples espacios por uno.
    """
    t = html.unescape(str(txt))
    t = re.sub(r'<[^>]+>', ' ', t)          # eliminar HTML
    t = re.sub(r'https?://\S+', ' ', t)   # eliminar URLs
    t = unicodedata.normalize('NFKC', t).lower()
    return re.sub(r'\s+', ' ', t).strip()

# 3) Aplicar limpieza b√°sica
df['summary_clean'] = df['summary'].apply(clean_text)
print("üîç Primeros ejemplos de `summary_clean`:")
for sample in df['summary_clean'].head(3):
    print(" ‚Ä¢", sample)

# 4) Definir stopwords manuales üõë
#    - Stopwords en ingl√©s de sklearn
#    - T√©rminos irrelevantes de dominio GDPR
#    - Nombres y siglas de agencias de protecci√≥n de datos
#    - Gentilicios / nacionalidades
#    - T√©rminos redundantes comunes en todos los clusters

# stopwords generales + dominio
domain_stop = set(ENGLISH_STOP_WORDS) | {
    'eur', 'fine', 'imposed', 'dpa', 'company', 'controller', 'processor'
}

# agencias DPA
agencies_stop = {
    'aepd', 'agpd', 'cnil', 'ico', 'edpb', 'garante',
    'bvd', 'datainspectorate', 'dpd', 'commission', 'authority',
    'commissioner', 'regulator'
}

# gentilicios / nacionalidades
demonyms_stop = {
    'french', 'german', 'italian', 'spanish', 'chilean',
    'polish', 'dutch', 'irish', 'portuguese', 'belgian',
    'austrian', 'romanian', 'bulgarian', 'croatian',
    'czech', 'slovakian', 'hungarian', 'swedish',
    'finnish', 'norwegian', 'danish', 'greek'
}

# ruido compartido de clusters
cluster_noise = {
    'data', 'personal', 'processing', 'information', 'gdpr',
    'subject', 'subjects', 'art', 'dpa', 'aepd', 'company'
}

# unificar todas las stopwords
total_stop = domain_stop | agencies_stop | demonyms_stop | cluster_noise
print(f"üõë Total stopwords manuales definidas: {len(total_stop)}")

# 5) Filtrado autom√°tico con TF-IDF
vectorizer = TfidfVectorizer(
    stop_words=list(total_stop),       # convertir set a lista
    lowercase=False,                   # ya est√° en min√∫sculas
    token_pattern=r"(?u)\b[a-z√°√©√≠√≥√∫√±√º]{2,}\b",
    max_df=0.70,                       # descarta t√©rminos en >70% de docs
    ngram_range=(1, 2)
)
# Ajustar para crear vocabulario
dtm = vectorizer.fit_transform(df['summary_clean'])
# Extraer vocabulario depurado
good_terms = set(vectorizer.get_feature_names_out())

# 6) Reconstruir summary_tfidf a partir de vocabulario filtrado
def filter_vocab(txt):
    return ' '.join([w for w in txt.split() if w in good_terms])

df['summary_tfidf'] = df['summary_clean'].apply(filter_vocab)
print("üîç Primeros ejemplos de `summary_tfidf`:")
for sample in df['summary_tfidf'].head(3):
    print(" ‚Ä¢", sample)

# 7) Guardar DataFrame normalizado en Parquet
norm_path = os.path.join(OUTPUT_DIR, 'gdpr_norm.parquet')
df.to_parquet(norm_path, index=False)
print("üíæ Parquet normalizado guardado en:", norm_path)

# ‚ÑπÔ∏è Para replicar con otro dataset:
#    ‚Ä¢ Ajusta clean_parquet.
#    ‚Ä¢ Modifica clean_text seg√∫n nuevas necesidades.
#    ‚Ä¢ Enriquecer stopwords o par√°metros de vectorizer seg√∫n el dominio.

"""# Script 03 - Generaci√≥n de Embeddings con API (OpenAI)"""

# =========================================================================
# Script 3 ‚Äî Embeddings OpenAI (robusto, con sanitizaci√≥n y fallback)
# =========================================================================
import os, time, json, unicodedata
import numpy as np
import pandas as pd
from openai import OpenAI, BadRequestError, RateLimitError, APIConnectionError, APIStatusError

# --- Config ---
cfg          = json.load(open('/content/outputs/config.json'))
OUTPUT_DIR   = cfg['output_dir']
MODEL        = cfg.get('embedding_model', 'text-embedding-3-small')  # recomendado
BATCH_SIZE   = int(cfg.get('batch_size', 64))                         # baja para depurar
MAX_RETRIES  = int(cfg.get('max_retries', 5))
BACKOFF_BASE = int(cfg.get('backoff_base', 2))
MAX_CHARS    = int(cfg.get('max_chars_per_item', 8000))               # recorte conservador

print(f"‚öôÔ∏è  Modelo embeddings: {MODEL}")

# --- Datos ---
inp = os.path.join(OUTPUT_DIR, 'gdpr_norm.parquet')
df  = pd.read_parquet(inp)
print(f"üì• Le√≠dos {len(df)} textos desde: {inp}")

# --- Sanitizaci√≥n estricta ---
def sanitize(x: str) -> str:
    if not isinstance(x, str):
        x = "" if x is None else str(x)
    # quita NUL u otros controles problem√°ticos
    x = x.replace("\x00", " ")
    x = unicodedata.normalize("NFKC", x).strip().lower()
    if not x:
        x = "[no_summary]"
    # recorte conservador por longitud de caracteres
    if len(x) > MAX_CHARS:
        x = x[:MAX_CHARS]
    return x

texts = [sanitize(x) for x in df['summary_tfidf'].tolist()]

client = OpenAI()  # usa OPENAI_API_KEY del entorno

embeddings = []
bad_rows   = []    # para rastrear √≠tems problem√°ticos

def embed_batch(batch, batch_start_idx):
    """Intenta el batch; ante 400 descompone por √≠tem para identificar problem√°ticos."""
    try:
        r = client.embeddings.create(model=MODEL, input=batch)
        return [d.embedding for d in r.data], []
    except BadRequestError as e:
        # 400: no reintentar ciegamente; probar uno por uno
        print(f"üß™ Batch con 400, probando por √≠tem‚Ä¶ ({str(e)[:120]} ‚Ä¶)")
        ok_vecs, bads = [], []
        for j, txt in enumerate(batch):
            try:
                r1 = client.embeddings.create(model=MODEL, input=txt)
                ok_vecs.append(r1.data[0].embedding)
            except BadRequestError as e1:
                global_idx = batch_start_idx + j
                snippet = txt[:160].replace("\n", " ")
                print(f"   ‚ö†Ô∏è  √≠tem {global_idx} inv√°lido: {str(e1)[:100]} | '{snippet}‚Ä¶'")
                bads.append(global_idx)
            except (RateLimitError, APIConnectionError, APIStatusError) as e2:
                # errores transitorios por √≠tem: espera corta y reintenta una vez
                time.sleep(2)
                try:
                    r1 = client.embeddings.create(model=MODEL, input=txt)
                    ok_vecs.append(r1.data[0].embedding)
                except Exception as e3:
                    global_idx = batch_start_idx + j
                    print(f"   ‚ö†Ô∏è  √≠tem {global_idx} fall√≥ de nuevo: {e3}")
                    bads.append(global_idx)
        return ok_vecs, bads
    except (RateLimitError, APIConnectionError, APIStatusError) as e:
        # errores transitorios a nivel de batch: backoff y reintento
        raise e

total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE
for b in range(total_batches):
    start = b * BATCH_SIZE
    batch = texts[start:start + BATCH_SIZE]
    attempt = 0
    while True:
        try:
            vecs, bads = embed_batch(batch, start)
            embeddings.extend(vecs)
            bad_rows.extend(bads)
            print(f"‚úî Lote {b+1}/{total_batches} ‚Üí {len(vecs)} emb OK; {len(bads)} err")
            break
        except (RateLimitError, APIConnectionError, APIStatusError) as e:
            attempt += 1
            if attempt > MAX_RETRIES:
                raise RuntimeError(f"Batch {b+1} agot√≥ reintentos: {e}")
            wait = BACKOFF_BASE ** attempt
            print(f"‚è≥ Transitorio en lote {b+1}: {e}. Reintentando en {wait}s‚Ä¶")
            time.sleep(wait)

# Alinear tama√±os: si hubo bad_rows, elimina esas filas de df para mantener √≠ndice
if bad_rows:
    print(f"üö´ √çtems descartados por 400: {len(bad_rows)}")
    mask = np.ones(len(df), dtype=bool)
    mask[bad_rows] = False
    df = df.loc[mask].reset_index(drop=True)

# Guardado
emb_array = np.array(embeddings, dtype=np.float32)
emb_path  = os.path.join(OUTPUT_DIR, 'embeddings.npy')
np.save(emb_path, emb_array)
print(f"üíæ Embeddings ‚Üí {emb_path} ‚Äî shape: {emb_array.shape}")

df['emb_idx'] = np.arange(len(df))
out_df = os.path.join(OUTPUT_DIR, 'df_emb.parquet')
df.to_parquet(out_df, index=False)
print(f"üíæ DF ‚Üí {out_df}")

"""# Script 04 - Clustering K-Means"""

# =========================================================================
# Script 4 ‚Äî Clustering K‚ÄëMeans y selecci√≥n de K reproducible
#               (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace este script?
# Objetivos:
#   - Cargar los embeddings generados (embeddings.npy) y el DataFrame enlazado (df_emb.parquet).
#   - Evaluar la calidad de clustering para distintos valores de K usando la m√©trica silhouette.
#   - Seleccionar el K √≥ptimo donde la silhouette es m√°xima.
#   - Entrenar el modelo K‚ÄëMeans final y asignar cada documento a un cluster.
#   - Guardar el modelo entrenado, el DataFrame con clusters y los scores de silhouette.
#
# ¬øPor qu√©?
#   - K‚ÄëMeans permite agrupar documentos similares seg√∫n su embedding sem√°ntico.
#   - El an√°lisis de silhouette ayuda a determinar cu√°ntos clusters representan mejor la estructura de los datos.
#   - Al guardar el modelo y los resultados, se facilita su reutilizaci√≥n y evaluaci√≥n posterior.
#
# Requisitos:
#   - Haber ejecutado los Scripts¬†0,¬†1,¬†2 y¬†3 para disponer de:
#       ‚Ä¢ outputs/config.json
#       ‚Ä¢ outputs/embeddings.npy
#       ‚Ä¢ outputs/df_emb.parquet
#   - Tener instalada la librer√≠a scikit‚Äëlearn (se instal√≥ en Script¬†0).
#
# Flujo:
#   1) Cargar configuraci√≥n y datos.
#   2) Para K en el rango 4‚Äì16:
#        ‚Ä¢ Entrenar K‚ÄëMeans con random_state fijo.
#        ‚Ä¢ Calcular silhouette_score.
#        ‚Ä¢ Convertir el score a float nativo y almacenar.
#        ‚Ä¢ Imprimir la tabla K vs. silhouette.
#   3) Identificar el K √≥ptimo.
#   4) Entrenar K‚ÄëMeans final con el K √≥ptimo y asignar clusters.
#   5) Guardar:
#        ‚Ä¢ El objeto K‚ÄëMeans en kmeans.joblib.
#        ‚Ä¢ El DataFrame con la columna `cluster` en df_clustered.parquet.
#        ‚Ä¢ Los scores de silhouette en silhouette.json.
# =========================================================================

import os
import json

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import joblib

# 1) Carga de configuraci√≥n y datos
cfg      = json.load(open('/content/outputs/config.json'))
OUTPUT   = cfg['output_dir']
SEED     = cfg['seed']

# Cargar embeddings y DataFrame enlazado
X  = np.load(os.path.join(OUTPUT, 'embeddings.npy'))
df = pd.read_parquet(os.path.join(OUTPUT, 'df_emb.parquet'))
print(f"üì• Cargados embeddings (shape={X.shape}) y DataFrame (n={len(df)})")

# 2) Evaluar K √≥ptimo usando silhouette
scores = {}
print("K\tSilhouette")
for k in range(4, 17):
    km = KMeans(n_clusters=k, random_state=SEED, n_init='auto')
    labels = km.fit_predict(X)
    sil_score = float(silhouette_score(X, labels))  # convertir a float Python
    scores[k] = sil_score
    print(f"{k}\t{sil_score:.4f}")

# 3) Seleccionar el mejor K
best_k = max(scores, key=scores.get)
print(f"\nüìà K √≥ptimo = {best_k} (Silhouette={scores[best_k]:.4f})")

# 4) Entrenar el modelo final y asignar clusters
km_final = KMeans(n_clusters=best_k, random_state=SEED, n_init='auto')
df['cluster'] = km_final.fit_predict(X)

# 5) Guardar artefactos
model_path      = os.path.join(OUTPUT, 'kmeans.joblib')
df_path         = os.path.join(OUTPUT, 'df_clustered.parquet')
silhouette_path = os.path.join(OUTPUT, 'silhouette.json')

joblib.dump(km_final, model_path)
df.to_parquet(df_path, index=False)
with open(silhouette_path, 'w', encoding='utf-8') as f:
    json.dump(scores, f, indent=2)

print("üíæ Artefactos guardados:")
print(f"   ‚Ä¢ Modelo K‚ÄëMeans         : {model_path}")
print(f"   ‚Ä¢ DataFrame con clusters  : {df_path}")
print(f"   ‚Ä¢ Silhouette scores JSON: {silhouette_path}")

# ‚ÑπÔ∏è Para replicar o ajustar:
#   ‚Ä¢ Modifica el rango de K en el bucle (l√≠neas 27‚Äì28).
#   ‚Ä¢ Prueba otros algoritmos de clustering (reemplaza KMeans por AgglomerativeClustering, etc.).
#   ‚Ä¢ Cambia la semilla en config.json para explorar estabilidad.

"""# Script 05 - Perfilado Estad√≠stico de Clusters"""

# =========================================================================
# Script 5 ‚Äî Perfilado estad√≠stico de clusters reproducible
#               (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace este script?
# Objetivos:
#   - Cargar el DataFrame con clusters asignados.
#   - Tokenizar cada resumen para extraer palabras relevantes.
#   - Contar la frecuencia de tokens y obtener los top-N t√©rminos por cluster.
#   - Mostrar ejemplos representativos de cada grupo.
#   - Guardar un JSON con la informaci√≥n de perfil (tama√±o, top_terms, ejemplos).
#
# ¬øPor qu√©?
#   - Ayuda a interpretar la sem√°ntica de cada cluster.
#   - Facilita la validaci√≥n manual: ves los t√©rminos clave y ejemplos.
#   - Genera un artefacto reusable (`cluster_profile.json`) para informes o dashboards.
#
# Requisitos:
#   - Haber ejecutado los Scripts¬†0‚Äì4 para disponer de:
#       ‚Ä¢ outputs/config.json
#       ‚Ä¢ outputs/df_clustered.parquet
#   - Librer√≠as: pandas, numpy, sklearn (instaladas en Script¬†0).
#
# Flujo:
#   1) Cargar configuraci√≥n y el DataFrame final con clusters.
#   2) Definir funci√≥n `tokenize` basada en regex para extraer palabras.
#   3) Definir `top_terms` que cuenta tokens y devuelve los n m√°s comunes.
#   4) Recorrer cada cluster:
#        ‚Ä¢ Contar documentos.
#        ‚Ä¢ Extraer top_terms.
#        ‚Ä¢ Tomar 3 ejemplos de res√∫menes.
#   5) Imprimir en pantalla un resumen r√°pido de cada cluster.
#   6) Guardar el diccionario `profile` en `outputs/cluster_profile.json`.
# =========================================================================

import os
import json
import pandas as pd
from collections import Counter
import re

# 1) Carga de configuraci√≥n y datos
cfg = json.load(open('/content/outputs/config.json'))
OUTPUT_DIR = cfg['output_dir']
input_path = os.path.join(OUTPUT_DIR, 'df_clustered.parquet')
df = pd.read_parquet(input_path)
print(f"üì• Datos cargados: {input_path} (n={len(df)})\n")

# 2) Tokenizaci√≥n ligera con regex
def tokenize(text):
    """
    Extrae palabras alfab√©ticas de al menos 3 caracteres,
    incluyendo vocales acentuadas, en min√∫scula.
    """
    if not isinstance(text, str):
        return []
    return re.findall(r"[a-z√°√©√≠√≥√∫√±√º]{3,}", text.lower())

# 3) Funci√≥n para obtener los top-N t√©rminos
def top_terms(texts, n=10):
    cnt = Counter()
    for doc in texts:
        cnt.update(tokenize(doc))
    return [term for term, _ in cnt.most_common(n)]

# 4) Construir el perfil de cada cluster
profile = {}
for c in sorted(df['cluster'].unique()):
    cluster_id = int(c)
    docs = df.loc[df['cluster'] == c, 'summary_tfidf'].dropna().tolist()
    profile[cluster_id] = {
        'count': len(docs),
        'top_terms': top_terms(docs, 10),
        'examples': docs[:3]
    }

# 5) Mostrar resultados por pantalla
for cluster_id, info in profile.items():
    print(f"--- Cluster {cluster_id} (n={info['count']}) ---")
    print("Top terms :", info['top_terms'])
    print("Examples  :")
    for ex in info['examples']:
        print(" ‚Ä¢", ex)
    print()

# 6) Guardar perfil en JSON
output_path = os.path.join(OUTPUT_DIR, 'cluster_profile.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(profile, f, indent=2, ensure_ascii=False)
print(f"üíæ Perfil guardado en: {output_path}")

"""# Script 06 - Visualizaci√≥n 2D de Clusters"""

# =========================================================================
# Script 6 ‚Äî Visualizaci√≥n 2D de clusters reproducible
#               (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace este script?
# Objetivos:
#   - Reducir los embeddings de alta dimensi√≥n a 2D usando TruncatedSVD.
#   - Graficar cada cluster en un scatter plot para validar visualmente la separaci√≥n.
#   - Exportar un archivo de coordenadas (x,y,cluster) para dashboards o an√°lisis adicional.
#
# ¬øPor qu√©?
#   - La proyecci√≥n 2D facilita inspeccionar solapamientos o grupos bien definidos.
#   - Un gr√°fico es una herramienta r√°pida para comunicar resultados.
#   - Las coordenadas exportadas permiten crear visualizaciones interactivas en aplicaciones externas.
#
# Requisitos:
#   - Haber ejecutado los Scripts¬†0‚Äì5 para disponer de:
#       ‚Ä¢ outputs/config.json
#       ‚Ä¢ outputs/embeddings.npy
#       ‚Ä¢ outputs/df_clustered.parquet
#   - Tener instalada la librer√≠a scikit‚Äëlearn y matplotlib (v√≠a pip en Script¬†0).
#
# Flujo:
#   1) Cargar configuraci√≥n, embeddings y DataFrame con clusters.
#   2) Aplicar TruncatedSVD para proyectar a 2 dimensiones.
#   3) Generar un scatter plot coloreando por cluster.
#   4) Guardar la imagen como PNG.
#   5) Exportar un CSV con columnas x, y y cluster.
# =========================================================================

import os
import json

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD

# 1) Carga de configuraci√≥n y datos
cfg = json.load(open('/content/outputs/config.json'))
OUTPUT_DIR = cfg['output_dir']
EMB_PATH   = os.path.join(OUTPUT_DIR, 'embeddings.npy')
DF_PATH    = os.path.join(OUTPUT_DIR, 'df_clustered.parquet')

embs = np.load(EMB_PATH)
df   = pd.read_parquet(DF_PATH)
print(f"üì• Cargados embeddings {embs.shape} y DataFrame con {len(df)} registros")

# 2) Proyecci√≥n a 2 dimensiones
svd = TruncatedSVD(n_components=2, random_state=cfg['seed'])
xy  = svd.fit_transform(embs)
print("‚úÖ Proyecci√≥n 2D completada")

# 3) Gr√°fico de clusters
plt.figure(figsize=(8, 5))
for cluster_id in sorted(df['cluster'].unique()):
    mask = df['cluster'] == cluster_id
    plt.scatter(xy[mask, 0], xy[mask, 1], s=15, alpha=0.6, label=f'Cluster {cluster_id}')
plt.legend(bbox_to_anchor=(1, 1))
plt.title('Clusters GDPR (2D via SVD)')
plt.xlabel('Dimensi√≥n 1')
plt.ylabel('Dimensi√≥n 2')
plt.tight_layout()

# 4) Guardar la visualizaci√≥n
png_path = os.path.join(OUTPUT_DIR, 'clusters_2d.png')
plt.savefig(png_path, dpi=150)
print("üíæ Gr√°fico guardado en:", png_path)

# 5) Exportar coordenadas para dashboards
coords = pd.DataFrame(xy, columns=['x','y']).assign(cluster=df['cluster'])
csv_path = os.path.join(OUTPUT_DIR, 'clusters_2d.csv')
coords.to_csv(csv_path, index=False)
print("üíæ Coordenadas exportadas en CSV:", csv_path)

# ‚ÑπÔ∏è Para replicar o ajustar:
#   ‚Ä¢ Cambia TruncatedSVD por UMAP o PCA para probar otras proyecciones.
#   ‚Ä¢ Ajusta el tama√±o del plot o colores seg√∫n tu preferencia.

"""# Script 07 - Generaci√≥n Autom√°tica de Insights con API de OpenAI"""

# =========================================================================
# Script 7 ‚Äî Generaci√≥n de insights autom√°ticos sin mapeo manual
#               (para Google Colab 2024‚Äë25)
#
# ¬øQu√© hace?
#   1) Carga resultados de:
#        - cluster_profile.json
#        - silhouette.json
#        - df_clustered.parquet (para ejemplos)
#   2) Monta un prompt con:
#        ‚Ä¢ Silhouette scores.
#        ‚Ä¢ Perfiles de clusters: tama√±o, top terms y ejemplos (3).
#   3) Llama a la API de OpenAI con streaming para generar:
#        A) Patrones comunes.
#        B) Diferencias clave.
#        C) Recomendaciones pr√°cticas.
#   4) Imprime el informe completo y lo guarda en Markdown.
#
# Requisitos:
#   ‚Ä¢ Scripts¬†0‚Äì6 ya ejecutados:
#       - outputs/config.json
#       - outputs/cluster_profile.json
#       - outputs/silhouette.json
#       - outputs/df_clustered.parquet
#   ‚Ä¢ openai >= 1.0.0 instalado, y OPENAI_API_KEY en entorno.
# =========================================================================

import os
import json
import pandas as pd
from openai import OpenAI

# 1) Cargar configuraci√≥n y artefactos
cfg       = json.load(open('/content/outputs/config.json', 'r', encoding='utf-8'))
OUT       = cfg['output_dir']
profile   = json.load(open(os.path.join(OUT, 'cluster_profile.json'), 'r', encoding='utf-8'))
scores    = json.load(open(os.path.join(OUT, 'silhouette.json'),    'r', encoding='utf-8'))
df        = pd.read_parquet(os.path.join(OUT, 'df_clustered.parquet'))

# 2) Generar texto de perfiles de clusters
profile_text = ""
for cid, info in profile.items():
    # L√≠nea principal con tama√±o y top terms
    terms = ", ".join(info['top_terms'])
    profile_text += f"- Cluster {cid} (n={info['count']}): Top terms = [{terms}]\n"
    # Tres ejemplos representativos
    examples = info.get('examples', [])
    for ex in examples[:3]:
        profile_text += f"    ‚Ä¢ {ex}\n"
    profile_text += "\n"

# 3) Construir prompt usando s√≥lo datos objetivos
prompt = f"""
Eres un analista de datos jur√≠dicos experto. A continuaci√≥n tienes los resultados cuantitativos del an√°lisis:

1) Silhouette scores por n√∫mero de clusters:
{json.dumps(scores, ensure_ascii=False, indent=2)}

2) Perfiles de clusters:
{profile_text}

Genera un informe en espa√±ol que contenga:
A) Patrones comunes en los clusters, en formato de vi√±etas.
B) Diferencias clave entre clusters, en vi√±etas.
C) Recomendaciones pr√°cticas para mejorar el an√°lisis y la interpretaci√≥n.
Termina la secci√≥n de recomendaciones con la frase "Fin de recomendaciones.".
"""

# 4) Llamada a la API de OpenAI con streaming y alto l√≠mite de tokens
client     = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
model_name = "gpt-3.5-turbo-16k"  # o "gpt-4-32k" si tienes acceso

response = client.chat.completions.create(
    model=model_name,
    messages=[
        {"role": "system", "content": "Eres un analista s√©nior de datos jur√≠dicos."},
        {"role": "user",   "content": prompt}
    ],
    temperature=0.7,
    max_tokens=1500,
    stream=True
)

# 5) Recolectar, imprimir y guardar el informe completo
full_insight = ""
print("\n===== Informe de Insights Generado por OpenAI =====\n")
for chunk in response:
    piece = chunk.choices[0].delta.content or ""
    print(piece, end="", flush=True)
    full_insight += piece
print("\n===== Fin del Informe =====\n")

# 6) Guardar en Markdown
out_path = os.path.join(OUT, 'general_insights.md')
with open(out_path, 'w', encoding='utf-8') as f:
    f.write("# Informe de Insights del Pipeline\n\n")
    f.write(full_insight)
    f.write("\n\nFin de recomendaciones.\n")

print(f"üíæ Insight guardado en: {out_path}")